<!DOCTYPE html>
<html>

<head>
    <title>Java Code Display</title>
    <style>
        body,
        html {
            margin: 0;
            padding: 0;
            height: 100%;
            overflow: hidden;
        }

        textarea {
            width: 100%;
            height: 100%;
            padding: 20px;
            box-sizing: border-box;
            font-family: monospace;
            font-size: 14px;
            border: none;
            outline: none;
            resize: none;
        }
    </style>
</head>

<body>
    <textarea rows="20" cols="80">
To perform partition based clustering

        library(factoextra)
        data <- read.csv("iris.csv", row.names = 1)
        df <- scale(data)
        set.seed(112)
        # Perform hierarchical clustering
        fit <- hclust(dist(df), method = "ward.D2") # Ward's method for hierarchical clustering
        # Plot dendrogram
        plot(fit, hang = -1, main = "Dendrogram of Iris Data")
        # Determine optimal number of clusters using the elbow method
        Kmax <- 15
        WCSS <- rep(NA, Kmax)
        for (i in 1:Kmax) {
        clusters <- cutree(fit, k = i)
        centers <- aggregate(df, by = list(clusters), FUN = mean)[, -1]
        WCSS[i] <- sum((df - centers[clusters,])^2)
        }
        plot(1:Kmax, WCSS, type = "b", pch = 19, xlab = "Number of clusters", ylab = "Within-cluster
        sum of squares", main = "Elbow Method for Optimal Number of Clusters")
        # Perform hierarchical clustering with the optimal number of clusters
        optimal_k <- which.min(WCSS)
        clusters <- cutree(fit, k = optimal_k)
        table(clusters)
        fviz_cluster(list(data = df, cluster = clusters), geom = "point", stand = FALSE)

        ///////////////////////////////////

To perform Hierarchical Clustering

rm(list=ls())
row.names=1
data<-read.csv("USArrests.csv",row.names = 1)
df
df<-scale(data)
dissim<-dist(df,method='euclidean')
hierClust<-hclust(dissim,method='complete')
plot(hierClust)
cluster<-cutree(hierClust,k=4)
intall.packages("clValid")
library(clValid)
dunn(dissim,cluster)
rect.hclust(hierClust,k=4,border=2:4)
abline(h=4,col='red')
/////////////////////////////////////////

: To perform Gradient Descent Optimization

rm(list=ls())
data<-mtcars
GRADIENT.DESCENT<-function(y,x,alpha,conv_threshold,n,max_iter){
plot(x,y,col="blue",pch=20)
m<-runif(1,0,1)
c<-runif(1,0,1)
yhat<-m*x+c
MSE<-sum((y-yhat)^2)/n
converged=F
iterations=0
while(converged==F){
m_new<-m-alpha*((1/n)*(sum((yhat-y)*x)))
c_new<-c-alpha*((1/n)*(sum(yhat-y)))
m<-m_new
c<-c_new
yhat<-m*x+c
MSE_new<- sum((y-yhat)^2)/n
if(MSE-MSE_new<=conv_threshold){
abline(c,m)
converged=T
return(paste("Optimal intercept:",c,"Optimal slope:",m,"No
of iterations:",iterations,"MSE:",MSE_new))
}
iterations=iterations+1
if(iterations>=max_iter){
    abline(c,m)
converged=T
return(paste("Optimal intercept:",c,"Optimal slope:",m,"No
of iteration:",iterations,"MSE:",MSE_new))
}
}
}
GRADIENT.DESCENT(data$mpg,data$wt,0.25,0.001,length(data$mpg),2500
)
slr<-lm(mpg~wt,data=mtcars)
slr$coef
mpg_p<-predict(slr)
sqerr<-(data$mpg-mpg_p)^2
MSE.SLR<-sum(sqerr)/length(data$mpg)
//////////////////////////////////////////

: To perform Regression
rm(list = ls())
data<- mtcars
library(dplyr)
data<- sample_n(data,15)
library("ggplot2")
ggplot(data,aes(x=wt,y=mpg))+geom_point()
cor.test(data$wt,data$mpg)
slr = lm(mpg~wt, data)
summary(slr)
plot(slr$resid)
qqnorm(slr$resid)
mlr = lm(mpg~wt+gear, data)
summary(mlr)
plot(mlr$resid)
qqnorm(mlr$resid)

////////////////////////////////////

: To perform Support Vector Machines
rm(list=ls())
data = read.csv('social.csv')
library(dplyr)
data=sample_n(data,80)
data = data[3:5]
data$Purchased = factor(data$Purchased,levels=c(0,1))
data_TRAIN<-sample_n(data,0.9*length(data$Purchased))
data_TEST<-setdiff(data,data_TRAIN)
data_TRAIN[-3] <- scale(data_TRAIN[-3])
data_TEST[-3] <- scale(data_TEST[-3])
library(e1071)
SVMclassifier = svm(formula = Purchased ~ ., data = data_TRAIN,type = 'C-classification',
kernal = 'linear')
plot(SVMclassifier,data_TRAIN)
y_p = predict(SVMclassifier, newdata = data_TEST[-3])
library(caret)
library(ggplot2)
library(lattice)
confusionMatrix(table(y_p,data_TEST$Purchased))
(data_TEST$Purchased)
////////////////////////////////////////////////////

: To perform Support Analysis of Variance(ANOVA)
R CODE:
rm(list=ls())
data <- data.frame (sale.count=c(40,60,70,30,50,30,30,10,70,60,50,60,30,20,20),
type=c("Can- A", "Can-A", "Can-A", "Can-A", "Can-A", "Can-B", "Can-B",
"Can-B",
"Can- B", "Can-B", "Can-C", "Can-C", "Can-C", "Can-C", "Can-C"))
library (dplyr)
group_by(data, type) %>% summarise(count = n(), mean = mean (sale.count, na.rm =
TRUE))
result <- aov(sale.count~type, data = data)
summary (result)
data<- PlantGrowth
group_by(data, group) %>% summarise (count = n(), mean = mean (weight, na.rm = TRUE))
result<- aov (weight~group, data = data)
summary(result)
TukeyHSD(result)
plot(result,1)
plot(result,2)
kruskal.test(weight~group, data = data)
/////////////////////////////////////////////

To perform Time Series Forecasting
rm(list=ls())
vec=c(3016, 3044, 3041, 3121, 3111, 3043, 2977, 3036, 3051, 3191,
3016, 3164, 3321, 3338, 3170, 3194, 3212, 3420, 3465, 3866, 3774,
3858, 3807, 3922, 4105, 4141, 4383, 4587, 4656, 4864, 5373, 5179,
5068, 5071, 4814, 5024)
data<- ts(vec, start=c(2018,1), end=c(2020,12), frequency=12)
start(data)
end(data)
frequency(data)
cycle(data)
summary(data)
plot(data)
abline(reg=lm(data~time(data)), col="red")
monthplot(data)
plot(aggregate(data, FUN=mean))
boxplot(data~cycle(data))
library(forecast)
seasonplot(data)
acf(data)
pacf(data, lag=length(data), p1=TRUE)
fit<- arima(data, order=c(3,2,2))
accuracy(fit)
newdata<- forecast(fit,4)
plot(newdata)
fit<- auto.arima(data)
newdata<- forecast(fit,4)
plot(newdata)
//////////////////////////////////////////

To perform Regression and forecasting on weather data
rm(list=ls())
a<-read.csv("weatherHistory2016.csv")
mlr = lm(Temperature..C.~Apparent.Temperature..C.+Humidity+Wind.Speed..km.h., a)
summary(mlr)
qqnorm(mlr$resid)
data<-ts(a$Temperature..C, start = as.Date("2016-01-01"),end = as.Date("2016-12-31"),
frequency = 24)
frequency(data)
summary(data)
plot(data)
plot(aggregate(data,FUN = mean))
boxplot(data~cycle(data))
library(forecast)
acf(data)
fit<-auto.arima(data)
accuracy(fit)
newdata<-forecast(fit,240)
plot(newdata)
////////////////////////////////////////////

To perform Logistic Regression
rm(list=ls())
data<-
read.csv("C:/Users/dsplab/Downloads/CreditWorthiness_TRAIN.csv",st
ringsAsFactors = T)
logreg<- glm(formula = data$creditScore ~.,family='binomial', data
= data) summary
(logreg)
logitrain<- predict(logreg, type='response') plot(logitrain)
tapply(logitrain,data$creditScore,mean) TEST_data<-
read.csv("C:/Users/dsplab/Downloads/CreditWorthiness_TEST.csv",str
ingsAsFactors = T)
logitest<- predict(logreg, newdata = TEST_data, type='response')
plot (logitest)
tapply(logitest,TEST_data$creditScore,mean)
TEST_data[logitest<=0.7, "LogiTest"]="bad"
TEST_data[logitest>0.7, "LogiTest"]="good"
# install.packages("caret") library
(caret)
confusionMatrix(table(TEST_data[,5],TEST_data[,6]),positive='good'
)
data<- read.csv("iris.csv",row.names=1)
////////////////////////////////////////////////

To perform K-NEAREST NEIGBOR CLASSIFIER
rm(list=ls())
data <- read.csv("CreditWorthiness (1).csv", stringsAsFactors = TRUE)
str(data)
summary(data)
plot(data)
data$Cdur <- as.integer(data$Cdur)
data$Cpur <- as.integer(data$Cpur)
data$Camt <- as.integer(data$Camt)
data$age <- as.integer(data$age)
data[, -5] <- scale(data[, -5])
set.seed(123)
train_indices <- sample(nrow(data), 900)
data_TRAIN <- data[train_indices, ]
data_TEST <- data[-train_indices, ]
library(class)
library(caret)
knnpredict <- knn(train = data_TRAIN[, -5], test = data_TEST[, -5], cl
= data_TRAIN$creditScore, k=5)
confusionMatrix(table(knnpredict, data_TEST$creditScore), positive =
'good')


    </textarea>
</body>

</html>